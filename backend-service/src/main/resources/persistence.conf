akka.persistence.r2dbc.connection-factory = ${akka.persistence.r2dbc.postgres}
akka {
  persistence {
    journal.plugin = "akka.persistence.r2dbc.journal"
    snapshot-store.plugin = "akka.persistence.r2dbc.snapshot"
    state.plugin = "akka.persistence.r2dbc.state"
    r2dbc {
      connection-factory {
//        url = "r2dbc:postgresql://schmidt:schmidt@localhost:5432/schmidt"
        url = "r2dbc:postgresql://schmidt:schmidt@localhost:5432/schmidt"
        ssl {
          enabled = off
        }
        # Initial pool size.
        initial-size = 3
        # Maximum pool size.
        max-size = 3
      }

      journal {
        class = "akka.persistence.r2dbc.journal.R2dbcJournal"

        # name of the table to use for events
        table = "event_journal"

        # the column type to use for event payloads (BYTEA or JSONB)
        payload-column-type = "BYTEA"

        # Otherwise it would be a pinned dispatcher, see https://github.com/akka/akka/issues/31058
        plugin-dispatcher = "akka.actor.default-dispatcher"

        # event replay is using akka.persistence.r2dbc.query.buffer-size

        # Set this to off to disable publishing of of events as Akka messages to running
        # eventsBySlices queries.
        # Tradeoff is more CPU and network resources that are used. The events
        # must still be retrieved from the database, but at a lower polling frequency,
        # because delivery of published messages are not guaranteed.
        # When this feature is enabled it will measure the throughput and automatically
        # disable/enable if the throughput exceeds the configured threshold. See
        # publish-events-dynamic configuration.
        publish-events = on

        # When publish-events is enabled it will measure the throughput and automatically
        # disable/enable if the throughput exceeds the configured threshold.
        # This configuration cannot be defined per journal, but is global for the ActorSystem.
        publish-events-dynamic {
          # If exponentially weighted moving average of measured throughput exceeds this
          # threshold publishing of events is disabled. It is enabled again when lower than
          # the threshold.
          throughput-threshold = 400
          # The interval of the throughput measurements.
          throughput-collect-interval = 10 seconds
        }

        # Group the slices for an entity type into this number of topics. Most efficient is to use
        # the same number as number of projection instances. If configured to less than the number of
        # of projection instances the overhead is that events will be sent more than once and discarded
        # on the destination side. If configured to more than the number of projection instances
        # the events will only be sent once but there is a risk of exceeding the limits of number
        # of topics that PubSub can handle (e.g. OversizedPayloadException).
        # Must be between 1 and 1024 and a whole number divisor of 1024 (number of slices).
        # This configuration can be changed in a rolling update, but there might be some events
        # that are not delivered via the pub-sub path and instead delivered later by the queries.
        # This configuration cannot be defined per journal, but is global for the ActorSystem.
        publish-events-number-of-topics = 128

        # replay filter not needed for this plugin
        replay-filter.mode = off
      }

      snapshot {
        class = "akka.persistence.r2dbc.snapshot.R2dbcSnapshotStore"
        table = "snapshot"

        # the column type to use for snapshot payloads (bytea or jsonb)
        payload-column-type = "BYTEA"

        # Otherwise it would be a pinned dispatcher, see https://github.com/akka/akka/issues/31058
        plugin-dispatcher = "akka.actor.default-dispatcher"

        # Enables an optimization in Akka for avoiding snapshot deletes in retention.
        only-one-snapshot = true
      }

      # Durable state store
      state {
        class = "akka.persistence.r2dbc.state.R2dbcDurableStateStoreProvider"

        table = "durable_state"

        # the column type to use for durable state payloads (bytea or jsonb)
        payload-column-type = "BYTEA"

        # When this is enabled the updates verifies that the revision is +1 of
        # previous revision. There might be a small performance gain if
        # this is disabled.
        assert-single-writer = on

        # Extract a field from the state and store in an additional database column.
        # Primary use case is for secondary indexes that can be queried.
        # Each entity type can have several additional columns.
        # The AdditionalColumn implementation may optionally define an ActorSystem
        # constructor parameter.
        additional-columns {
          #"<entity-type-name>" = ["<fqcn of AdditionalColumn implementation>"]
        }

        # Use another table for the given entity types. Typically used together with
        # additional-columns but can also be used without addition-columns.
        custom-table {
          #"<entity-type-name>" =  <other_durable_state_table>
        }

        # Additional processing in the same transaction as the Durable State upsert
        # or delete. Primary use case is for storing a query or aggregate representation
        # in a separate table.
        # The ChangeHandler implementation may optionally define an ActorSystem
        # constructor parameter.
        change-handler {
          #<entity-type-name>" = "<fqcn of ChangeHandler implementation>"
        }

      }

    }
  }

  projection.r2dbc {
    offset-store {
      # only timestamp based offsets
      offset-table = ""
    }
  }
}